{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word2Vec from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "The Word2Vec algorithm is a clever implementation of a more general Neural Network.\n",
    "\n",
    "Neural Networks can be thought of as <i>function approximators</i>. Oftentimes, we have inputs and outputs that can be represented as vectors but do not know the function that transforms input to output even though we would like to reproduce it. For example, if we throw a ball directly upward, we already know based on Newtonian physics that its height will be a function of time as determined by gravity and the initial velocity, and that we can identify its path in terms of these. However, without knowing anything about these features of motion, a Neural Network could be used to reverse engineer the path of the ball by showing it a series of inputs (times) and their associated outputs (heights). In turn, the Neural Network would be able to predict the ball's height at later times or in between the times it had been shown.\n",
    "\n",
    "Neural Networks are very often used to approximate functions that identify whether images have cats in them.\n",
    "\n",
    "At the grittiest mathematical level, Neural Networks consist of (at least!) two matrices. For convenience, we'll call them <i>Matrix A</i> and <i>Matrix B</i>. Without going into the Linear Algebra, the input vector is initially multiplied by Matrix A, creating a new vector that is then multiplied by Matrix B, producing the network's output vector. And that's it. The key here are the values contained in each of the matrices.\n",
    "\n",
    "The values of the network's matrices are learned through an iterative, correction process. Initially, the matrices are assigned random values, so naturally the network's output is random as well. However, that intial output gets compared to a desired, target vector and based on its error, the matrices get tweaked. In our previous example, the network would have been trained by showing it a given time (say, 2 seconds after throwing the ball) and comparing its predicted output (say, a randomly chosen -127 feet above ground) to the target output (say, 11 feet above ground). The values of Matrix B would first need to be revised in order to produce the correct output and the values of Matrix A would be revised based on those of Matrix B. Further pairs of inputs and outputs would be shown to the Network and the values of each matrix further updated.\n",
    "\n",
    "Once the network has been trained, we might think of Matrix A as encoding relationships among inputs and Matrix B as decoding these into outputs. Word2Vec takes advantage of the fact that when we train a Neural Network on CBOW or Skip-Gram models of language, each line of Matrix A represents a unique word's projection into semantic space.\n",
    "\n",
    "### A Note About Terminology\n",
    "I have used my own terms to describe the structure of a Neural Network, since they are simpler for our particular application. Networks are often described in terms of Layers, Nodes, and Weights.\n",
    "\n",
    "The input vector might be referred to as Layer 0, or the input layer. The vector that is produced by multiplying the input vector by Matrix A is Layer 1, or more often, the hidden layer. And the vector that is produced after multiplication with Matrix B is Layer 2, or the output layer. The network I have described has just two layers, but there may be potentially very many -- i.e. many hidden layers between the input and output. This is referred to as <i>deep learning</i>.\n",
    "\n",
    "Matrices A and B are typically referred to as Weight Matrices. The number of dimensions in them are referred to as nodes. The idea is that each node in each layer is connected to each node in the next one, akin to the neurons of a human brain. The matrices record the strength of connection between each node.\n",
    "\n",
    "### One-Hot Vector Representation of Words\n",
    "In order to make useful inputs and targets for our Neural Network, words initially get represented by a <i>one-hot vector</i>. Essentially, the one-hot encoding of a word is a document-term matrix for a single document with a single word and as many columns as there are unique words in the entire corpus. Every value for the document is zero except in the column for the word in question. This strategy is used to distinguish words from one another when they are represented as inputs and targets. Since the values of the vector are entirely independent of one another, no relationships between words get expressed by these.\n",
    "\n",
    "For CBOW, the target output is simply the one-hot vector of the word in question. The input is the average of the one-hot vectors representing context words within a given window.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Vocabulary: Most frequent words to include in our vocabulary; determines size of one-hot word vectors and, in turn, one dimensions of  matrices</li>\n",
    "<li>Semantic Space: Dimensions in which words' relationships get encoded; determines other dimension of matrices; alternately referred to as the number of <i>hidden nodes</i></li>\n",
    "<li>Window: Context words to observe left and right</li>\n",
    "<li>Alpha: learning rate; prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Epochs: Passes through dataset</li>\n",
    "<li>Batch Size: Words to sample from data during each pass</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set value for each feature\n",
    "\n",
    "vocab_size = 10000\n",
    "semantic_space_size = 100\n",
    "window = 5\n",
    "alpha = 1e-4\n",
    "epochs = 5\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each unique token to a column in the one-hot vector\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "all_tokens = [token for sentence in words_by_sentence for token in sentence]\n",
    "token_counts = Counter(all_tokens)\n",
    "common_tokens = [token for token,frequency in token_counts.most_common(vocab_size-1)]\n",
    "\n",
    "# We'll add a dummy term to represent all infrequent words\n",
    "common_tokens.append('_SLUG_')\n",
    "\n",
    "token_to_ix = { tk:i for i,tk in enumerate(common_tokens) }\n",
    "ix_to_token = { i:tk for i,tk in enumerate(common_tokens) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "token_to_ix['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "ix_to_token[1618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matrices with random values\n",
    "\n",
    "Matrix_A = np.random.randn(vocab_size,semantic_space_size)*0.01\n",
    "Matrix_B = np.random.randn(semantic_space_size,vocab_size)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "Matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need two functions:\n",
    "# 1) Produce appropriate input and output vectors for our CBOW model, given a list of tokens in a sentence\n",
    "# 2) Update the values in our matrices after each observation\n",
    "\n",
    "def CBOW_input_output(sentence):\n",
    "    \n",
    "    # Get one-hot values for each word in sentence\n",
    "    ix_map = [token_to_ix[word] if word in common_tokens else token_to_ix['_SLUG_'] for word in sentence]\n",
    "    \n",
    "    # Randomly select a target word\n",
    "    target_index = np.random.choice(range(len(sentence)))\n",
    "    \n",
    "    \n",
    "    # Create (target) vector in which all values are zero\n",
    "    target_vector = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Set target-word value to 1\n",
    "    target_vector[ix_map[target_index]] = 1\n",
    "    \n",
    "    \n",
    "    # Create (input) vector in which all values are zero\n",
    "    input_vector = np.zeros((vocab_size,1))\n",
    "    \n",
    "    # Iterate through each of window-distance words to left of target\n",
    "    for i in range(target_index-window, target_index):\n",
    "        \n",
    "        # Add 1 to each word's value in the input vector\n",
    "        if i in range(len(sentence)) and sentence[i] in common_tokens:\n",
    "            input_vector[ix_map[i]] += 1\n",
    "\n",
    "            \n",
    "    # Iterate through each of window-distance words to left of target\n",
    "    for i in range(target_index+1, target_index+window+1):\n",
    "        \n",
    "        # Add 1 to each word's value in the input vector\n",
    "        if i in range(len(sentence)) and sentence[i] in common_tokens:\n",
    "            input_vector[ix_map[i]] += 1\n",
    "        \n",
    "\n",
    "    \n",
    "    # Divide by number of words observed\n",
    "    input_vector = input_vector/(sum(input_vector)+1e-8)\n",
    "            \n",
    "    return input_vector, target_vector\n",
    "\n",
    "\n",
    "\n",
    "def train_function(input_vector_, target_vector_, Matrix_A_, Matrix_B_, alpha_):\n",
    "    \n",
    "    # Multiply input by matrices to produce output\n",
    "    hidden_vector = np.dot(input_vector_.T,Matrix_A_)\n",
    "    output_vector = 1/(1+np.exp(-(np.dot(hidden_vector,Matrix_B_))))\n",
    "    \n",
    "    # Note that after the output vector is produced, it is put into an activation function\n",
    "    # -- in this case, the logistic function -- that squashes its values between (0,1).\n",
    "    # This basically measures each vocabulary word's probability of being the true output.\n",
    "    \n",
    "    # Determine error for each matrix\n",
    "    output_delta = (target_vector_.T - output_vector)\n",
    "    hidden_delta =  np.dot(output_delta,Matrix_B_.T)\n",
    "\n",
    "    # Update matrix values\n",
    "    Matrix_B_ += np.dot(hidden_vector.T,output_delta)*alpha_\n",
    "    Matrix_A_ += np.dot(input_vector_,hidden_delta)*alpha_\n",
    "    \n",
    "    return(Matrix_A_,Matrix_B_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our Neural Network\n",
    "\n",
    "for _ in range(epochs):\n",
    "    sampled_sentences = np.random.choice(words_by_sentence, batch_size)\n",
    "    for sentence in sampled_sentences:\n",
    "        input_vector, target_vector = CBOW_input_output(sentence)\n",
    "        Matrix_A, Matrix_B = train_function(input_vector, target_vector, Matrix_A, Matrix_B, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "Matrix_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each matrix row\n",
    "\n",
    "row_norm = np.linalg.norm(Matrix_A, axis=1)\n",
    "Matrix_A_norm = Matrix_A / row_norm[:, numpy.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to retrieve word embeddings, determine similarities\n",
    "\n",
    "def get_vector(word,embedding_matrix=Matrix_A_norm):\n",
    "    if word in common_tokens:\n",
    "        ix = token_to_ix[word]\n",
    "        return embedding_matrix[ix]\n",
    "    else:\n",
    "        print(word, \"not in vocabulary\")\n",
    "    \n",
    "\n",
    "def token_similarity(token_1,token_2):\n",
    "    vector_1 = get_vector(token_1)\n",
    "    vector_2 = get_vector(token_2)\n",
    "    return 1-cosine(vector_1,vector_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve word embedding\n",
    "get_vector('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pronouns should be close to one another\n",
    "token_similarity('she','her')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare a pronoun to a control word with a different POS\n",
    "token_similarity('call','me')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we recreate the analogy 'king' - 'man' + 'woman' ~ 'queen?\n",
    "# This formulation plays with some of the underlying algebra\n",
    "\n",
    "token_similarity('king','queen') - token_similarity('man','queen') + token_similarity('king','queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EX. Write a function that will produce inputs and outputs for the Skip-Gram model.\n",
    "\n",
    "## EX. Our model randomly selects words from the corpus, so it implicitly prefers frequent tokens.\n",
    "##     Modify the script to minimize (but not eliminate) appearances of high-frequency terms.\n",
    "##     as components of input vectors.\n",
    "\n",
    "## EX. It has been found that the learning process can be sped up by showing \"wrong\" targets\n",
    "##     during training and penalizing the network when it predicts them as outputs.\n",
    "##     Implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Vector Rejection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions that reject vectors, find nearest words\n",
    "\n",
    "def vector_rejector(vector_1,vector_2):\n",
    "    # Rejects vector_2 from vector_1\n",
    "    # Note that vectors are passed in, not words; also mean of\n",
    "    # several vectors could be passed as a single argument\n",
    "    vector_2_norm = vector_2 / np.linalg.norm(vector_2)\n",
    "    projection = np.dot(vector_1,vector_2_norm) * vector_2_norm\n",
    "    return vector_1 - projection\n",
    "\n",
    "def nearest_words(vector,embedding_matrix=Matrix_A_norm):\n",
    "    # Note that this relies on our 'ix_to_token' dictionary;\n",
    "    # format of embedding matrix is 2-D numpy array\n",
    "    new_matrix = np.dot(embedding_matrix, vector)\n",
    "    nearest_rows  = gensim.matutils.argsort(new_matrix, topn = 10, reverse = True)\n",
    "    result = [(ix_to_token[row_id], float(new_matrix[row_id])) for row_id in nearest_rows]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "she_vector = vector_rejector(get_vector('she'),get_vector('he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "she_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_words(she_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
