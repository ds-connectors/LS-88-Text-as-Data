{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from htrc_features import FeatureReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre classification with HTRC data\n",
    "\n",
    "In this example, we'll be classifying texts into 2 different genres: poetry and science-fiction. JSON files containing the metadata for 100 texts in each genre need to be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_output = !htid2rsync --f data/poetry.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ data/poetry/\n",
    "scifi_output = !htid2rsync --f data/scifi.txt | rsync -azv --files-from=- data.sharc.hathitrust.org::features/ data/scifi/\n",
    "\n",
    "outputs = list([poetry_output, scifi_output])\n",
    "subjects = ['poetry', 'scifi']\n",
    "\n",
    "paths = {}\n",
    "suffix = '.json.bz2'\n",
    "for subject, output in zip(subjects, outputs):\n",
    "    folder = subject\n",
    "    filePaths = [path for path in output if path.endswith(suffix)]\n",
    "    paths[subject] = [os.path.join(folder, path) for path in filePaths]\n",
    "    fn = 'data/' + subject + '_paths.txt'\n",
    "    with open(fn, 'w') as f:\n",
    "        for path in paths[subject]:\n",
    "            p = str(path) + '\\n'\n",
    "            f.write(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebooks, we'll construct `FeatureReader` objects for each corpus. The line below reads in path files we created to the downloaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {}\n",
    "subjects = ['poetry', 'scifi']\n",
    "for subject in subjects:\n",
    "    with open('data/' + subject + '_paths.txt', 'r') as f:\n",
    "        paths[subject] = ['data/' + line[:len(line)-1] for line in f.readlines()]\n",
    "        \n",
    "poetry = FeatureReader(paths['poetry'])\n",
    "scifi = FeatureReader(paths['scifi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our bag of words matrix, we need to keep a global dictionary of all words seen in each of our texts. We initialize \"wordDict\", which tracks all the words seen and records its index in the bag of words matrix. We also keep a list of volumes so that we can parse them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordDict(HTRC_FeatureReader_List):\n",
    "\n",
    "    wordDict = {}\n",
    "    i = 0 \n",
    "    volumes = []\n",
    "\n",
    "    for f in HTRC_FeatureReader_List:\n",
    "\n",
    "        for vol in f.volumes():\n",
    "            \n",
    "            volumes.append(vol)\n",
    "\n",
    "            tok_list = vol.tokenlist(pages=False)\n",
    "            tokens = tok_list.index.get_level_values('token')\n",
    "\n",
    "            for token in tokens:\n",
    "                if token not in wordDict.keys():\n",
    "                    wordDict[token] = i\n",
    "                    i += 1\n",
    "    \n",
    "    return wordDict, volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDict, volumes = createWordDict([scifi, poetry])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we construct the global dictionary, we can fill the bag of words matrix with the word counts for each volume. Once we have this, we will use it to format the training data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = np.zeros((200, len(wordDict.keys())))\n",
    "\n",
    "for i, vol in enumerate(volumes):\n",
    "    tok_list = vol.tokenlist(pages=False)\n",
    "    counts = list(tok_list['count'])\n",
    "    tokens = tok_list.index.get_level_values('token')\n",
    "    \n",
    "    for token, count in zip(tokens, counts):\n",
    "        try:\n",
    "            index = wordDict[token]\n",
    "            dtm[i, index] = count\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "X = dtm\n",
    "y = np.zeros((200))\n",
    "y[100:200] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the `TfidfTransformer` to format the bag of words matrix, so that we can fit it to our LinearSVC model. Let's see how our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import cross_validation\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "out = tfidf.fit_transform(X, y)\n",
    "\n",
    "model = LinearSVC()\n",
    "\n",
    "score = cross_validation.cross_val_score(model, X, y, cv=10)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the most helpful features, or words, for each class. First we'll `fit` the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.argsort(model.coef_[0])[:50]\n",
    "top_scifi = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_scifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feats = np.argsort(model.coef_[0])[-50:]\n",
    "top_poetry = [(list(feats).index(wordDict[w]) + 1, w) for w in wordDict.keys() if wordDict[w] in feats]\n",
    "sorted(top_poetry, key=lambda tup: tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
